{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Things-Learned I'm on a journey to learn all things Linux, DevOps, SysAdmin, Security, InFoSec & more. On this journey, I have found it super hard to understand things sometimes- if not all the time. The tech community has a tendency to write things in a way that assumes the reader is at a certain level, usually moderate to proficient. This repo will double as not only my notes (to reference at any time in my lifetime) but as a place for you, if you too find yourself in new territory hopefully this will help. I can only cover what I am exposed to, if there is something you would like to work on together, please let me know- I am happy and eager to learn!","title":"Introduction"},{"location":"%23%20Why%3F/","text":"VPN VPN and servers can be confusing, I am still confused by them today. Below are some notes that I have gathered. VPN If you ever have an issue with a connection one of the common places to check is your resolv.conf file. How? In your terminal with your editor of choice open resolv.conf, if you do not have this file the editor will automatically create it. You will want to add the namesever of the site you're trying to reach. Example: nameserver 172.X.XX.X.X When not needed feel free to comment it out. Example: #nameserver 172.X.XX.X.X The system wont read it. Once you have made this change, you will want to make sure that in your VPN client you go to the configuration settings and select something manual along the lines of \"allow changes manually to network changes\". ## External HardDrive When you have to move files, for example a tarball you need to execute the following command: tar -cvfz [NAME OF TARBALL].tar.gz [DESIRED LOCATION OF FILE] Connecting To Servers Working on systems will have you remoting into several machines. At times you will have to copy over tarballs from one machine into yours or your desired external storage device. For this you will use the scp command. In some cases your machine will not have a direct path to your key when you must list it out the command is as follows: scp -p 443 -i [PATH TO KEY] This command lists the port also to copy and where the key is to connect to the server. An example of copying contents from one server to an external hard drive would be: scp -p 443 -i ~/.ssh/[CERTIFICATE LOCATION] USERNAME@SEVERNAME:[LOCATION OF TARBALL][LOCATION WHERE YOU WANT IT COPIED] If you're lost on where to find the file path to your external drive (if thats where you want to push documents) Mac: Open the Disk Utility in the /Applications/Utilities/ folder, click on the partition, choose Info, and look at the disk identifier. Example: `Volumes/\"My Passport for Mac\" Linux: To be continued Windows: if you click on computer, then click the external drive in question, at the top of the window it will show you the path. Example: `C: Users\\sunflowerno0b\\USB Yes, this is very unique and its not commonly found, normally you indicate the scp command connect to the server and location of where you want to contents copied. 2FA Reset for OpenVPN VPNs allow for secure access to certain sites for your systems team. For example some teams may need only internal members to access your git. Authentication to these VPN protected sites normally include: ssl certificate username & password MFA When MFA needs to be rest, you should always look at the documentation. To rest the 2FA for a specific user using OpenVPN: ssh into the VPN (1XX.XX.XXX.XX) go into the root directory cd / run find / | grep sacli cd into the scripts folder run sudo ./scali -- user [USERNAME] --KEY \"prop_google_auth\" -- value \"false\" UserProPut Note that this will disable the google authenticator for the specific user. run sudo ./sacli -- user[USERNAME] -- lock 0 Google AuthLock this will reset the Google Authenticator for the user to rescan upon signing in again. Further options can be explored here VPN & LDAP When configuring VPN that will use your teams LDAP it is best practices for the username to match the LDAP profile when creating the user in admin. Should a user lose or replace phone","title":"VPN"},{"location":"Aleph/","text":"Intro to Aleph My organization needed to have a database repository in which journalist around the world could access and share documents surrounding findings about environmental matters. After a few discussions the service Aleph was decided to be the option that we went with. I will detail the steps below- this will be long so sit back, and prepare to read. In order to run Aleph it would be easiest to manage via an AWS EC2 instance. To perform the creation of the EC2 I did the following steps: 1) https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LaunchInstanceWizard : and select the server that you wanted- for this case I selected Ubuntu 20.05 LTS Server with the 64 bit x86. From there you are taking to step 2 out of 7 which is selecting the instance type. Here we have to see how many CPuS, RAM memory (GiB) would be need. I was confused about price so my friend mentioned that I can gather estimates from: https://calculator.aws/#/createCalculator/EC2 when using that page its easier to just type EC2 rather than the drop down menu because the services to get estimates on was endless. I selected the region to be US E. N Virginia - for no particular reason that a) at my old organization thats what they used b) we had another instance running there already and I wanted everything to be accessed at once. I then selected linux then ticked the search by instances by name - in this case t3.medium . I kept the utilization at 100% and the number of EC2 at 1. My friend explained that we have options for this, we could have auto scaling which means that there would be 3 instances so should one fail the other two could still keep the service running. But since this is just staging 1 would be enough for now. Total estimated cost per year would be 288USD. Note: t2 and t3 medium offered the same options at the same price (or not that much difference in price) so I opted for t3 as it was newer. I spoke to the Data person at my organization and he states that currently there is 25GB of data with room for plenty more. The next step (3) was configuring the instance details, at the juncture since its only 1 instance we need I left 1 as the instance count. I kept the network the same but made the subnet be subnet 1a, the reason for this is should this fail the subnet will provide redundancy. I enabled auto-assign public IP, and I kept the rest f the configurations at defaults. Things to note: - the IAM role used to give permissions to edit or modify was resources from within the instance. -Shutdown behavior I left it at stop, not terminate because I don\u2019t want anything deleted/ I also added protect against accidental termination because since I am a no0b- in the event I mess up, I don\u2019t want all my work to be erased. -For monitoring through Cloudwatch, I left this unchecked as most of the moniotorizat ion options that come basic are enough for this staging. Adding storage: Here I needed to create thee volumes, by default AWS will create the root being dev/sda1 with a 8GB not encrypted. I added encryption and kept things as is. I then added two new volumes, 1 for 8 GB which would be where Docker lives and another with 100gb where the data would live. All were encrypted. For tags I created one which everyone should have at minimum which was NAME Aleph Staging. The configuration of security groups is where you add your rules. We added three 1 ssh (port 22), 1 http port (80) and 1 for HTTPS(443). Once the instance is up, I will want to change the port that SSH listens on to reduce attack zone. I will also want to make custom changes to the IP here so that again the attack zone is minimized. Once I reviewed everything, I was able to launch the instance. AWS asked if I wanted to store my key or create a new one. AWS charges 10$/key (which is crazy) so I used the Pem file that was created in a previous issue from that contractor. Once I login, I can add my own key so that I do not have to use the generic key that we have for the other instance. On my instances page I was able to see the public IPV4 address, this allowed me to Now that the instance is up and running, I have to ssh into the system, format the disks, mount them permanently, download docker and follow the installation instructions for Aleph. Formatting the disks: I used the fdisk -l command to list all the disks that I have within my server. From there I was able to get the names of each of the disks, most online blogs and tutorials will have /dev/sdb1 as the default- but that was not the case for me. It was nvme1n1 or some vacation of that for the three disks that I had. i ran sudo mkfs -t ext4 [NAME OF THE PARTITION OR DISK] then I verified that the file system was changed by running lsblk -f , my output was updated to ext4. https://phoenixnap.com/kb/linux-format-disk Mounting permanently: When mounting the disks that I created, as a refresher I have one of 8GiB and one of 10GiB- I have to select a mount point but this will roll out in several instances. The mount point can be of any name. Most of the examples I found had weird names or led me down understanding the linux file system. I did have to understand the linux file system but I will explain that later. To keep things simple I decided that I will create a mount point /data for the 100GiB. The command I ran to mount is mount /dev/[VOLUME NAME] /data . This will mount that disk to the mount point /data. To get the name of the volume data, I ran both lsblk -l & df -h . here I was able to read the output and ID each volume. Once it was mounted, if I reboot the volume would not be mounted and that would be a problem. So I need to mount permanently. To do that, I had to edit the etc/fstab file. The contents of the fstab file were minimal. Note: you have to run this as sudo. What you need to enter is something similar to the following: UUID=847bc509-b856-4a1e-9a01-c902bec56801 /scripts ext4 defaults,ro 0 0 To get the UUID for the disk I ran sudo blkid /dev/[DISK NAME] for me I added the nofail rule. This will prevent the instance from starting if the disks cannot mount. Once that file was edited, I saved it and then I reboot the system. To check that it was mounted in the /data point I ran yet again lsblk -l and I saw the disk in question was mounted. I performed the same for the other disk but this should be discussed in depth as it relates with Docker. Docker by default writes to the /var/lib/docker. I created the mount point as /Docker by default docker would not see this and not write here. It would perhaps break. I had to unmount the disK I had mounted to point /var/lib/Docker then I ran mv /var/lib/Docker/ /var/lib/docker . I had already installed Docker within that mount point. I was scared that the docker config files did not cary over with the name change. I ran sudo docker ps and I had output that indicated the information carried over. Now to explain Docker. Since docker will write to /var/lib/docker , in the linux file system, that path is connected to the root /. When I created the instance, I had three volumes: -root -data -docker so I had to make sure that the docker mount point was in the correct location so docker could run to the 10GB rather than the /file which was 8GB. The mount point in the fstab file pointed to /var/lib/docker. This ensures that my root stays clean and the 10GiB mounted to the path /var/lib/docker is what gets written to. https://stackoverflow.com/questions/40118442/how-to-change-mountpoint-name https://phoenixnap.com/kb/install-docker-on-ubuntu-20-04 Now that the disks were permanently mounted, docker installed I needed to look at the documentation to install Aleph. But on concern came to mind- what happens when our team exceeds the 100GiB, how do I increase the size of the volume. To increase the volume I went to my EC2 list, then clicked my instance ID, from there I went to storage. In storage I saw the volume size that I would be working with, this was the 100GiB. My friend said, as a test, let\u2019s increase it to 101. I clicked the Volume ID, then on the new page I clicked actions and I saw Modify volume. Here I could change the size. I entered 101 and clicked modify. I needed to verify that this change was made via terminal. I left my current session nd re ssh\u2019d into the machine. I ran df -hT as the documentation said- this did not help me. I ran lsblk -l and here I was able to see that it was 101 which is what I needed. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html?icmpid=docs_ec2_console CHANGING PORT ON AWS To change the port on aws, I needed to go into the instance /etc/sshd_config file. From there I needed to comment out the Port that I wanted to change it to. Afterwards, I needed to add the INBOUND rule to the security groups that I had for my instance. To do that, I went to my instance, clicked security group, deleted the default ssh 22 and added CUSTOM TCP, added my port number. Now I could run ssh -pXXXX [HOST NAME] but typing the port number each time would be annoying. So I had to edit my /etc/hosts file to add PORT XXXX. Once I saved my host file I was able to run service sshd restart and I opened a new terminal and I was able to ssh in. https://gist.github.com/tusharf5/89a094de01321880fdf44bf1d4e4ea4c https://serverfault.com/questions/338077/basic-ssh-port-change-not-working-on-ec2-instance ALEPH CONFIGURATION FILE: working within the aleph.ev file one of the first configuration options was to set the secret key. I ran openssl rand -hex 24 and pasted the output into the aleph.env configuration file. I then edited ALEPH APP tote to RIN Data and the slug to be rin_data. ELASTIC IP: as it stands now if i stop and restart the service the IP assigned to the instance will change. This is an issue because I created an A/AAA record with a set IP. If the IP is always changing I will have to go into Linode and change the IP address in the A/AAAA record. To prevent this it would be easier to assign a static IP or in aws world whats called an elastic IP. To do this I went to the Networking & Security portion of my ec2 dashboard. There I selected allocate Elastic UP address, I selected Amazons Pool of IPv4 addresses and added the tag for staging. After that I selected Allocate. Once the IP was addressed it had no name, I added the name Aleph staging. Then I clicked it and selected associate Elastic UP address. There was already one in use so I had to select the recently created one. After that the elastic IP was added. I then edited my /etc/hosts file because the IP was an old one, I changed it to the elastic IP address that I just generated. From there I also went into Linode and changed the A/AAA record to reflect the elastic IP address. Something to know about elastic ip\u2019s within AWS, so long as they are associated with an instance they are free. when they are not associated with on that is when you are charged. So if the service is no longer needed and has to be cut, a sysadmin must remember to disassociate the elastic ip. Also-since we have the name pulitzercenter.org that we pay for, it made no sense to pay 12 dollars a month to AWS to create the route53 record through their R53 wizard. This is why we went through A/AAA record as a subdomain, it would be cheaper to do. ADDING MY SSH KEY TO THE AWS SERVER: As of right now there was a pem file that I was using created by the contractor from other problems within aws to ssh in. What I wanted to do is to make sure that my set of keys could ssh in. So what I did: ran ssh-keygen -t rsa entered the password and the name then i ran chmod 600 on the .pub file , I then opened vim to copy the information of the new key. I ssh\u2019d into the server and I went to .ssh/authorized_keys, I opened the file and pasted the contents of my key there. I hit save. I then went back to my local machine and edited my /etc/hosts files to point my key instead of the generic ubuntu key on file. Then I was able to ssh in. ALEPH CONTINUED: After associating the elastic IP on AWS- I made the change to the /.ssh/config file but my friend said that I could just now point it to the staging website. So I attempted to change the config file but when I ran the ssh command it still did not work. we wanted to make sure that the dns record was updated and pointing correctly. we went to https://dnspropagation.net , i entered the staging website and I was able to see where it was available. Then I attempted to SSH once more. It did not work and this is still a mystery to be solved. install docker compose: https://docs.docker.com/compose/install/ INSTALLING SWAG ON SEVER FOR ALEPH: https://www.the-digital-life.com/webserver-linux/ The way I understand this at time of writing is that once we had the EC2 instance up, we had volumes / disks mounted to it and there was a CNAME attached to our server but there is essentially nothing on the server. In order to host the website we needed to install a web server within the server to support the docker compose images that we were installing. Thats when my friend showed me linuxserver/swag. It\u2019s an all in one web server that provides items like fail2ban, ssl certsmand ngnix all in one docker image. With my lack of know how this seems to be the best idea as it would do what I needed and then some. I followed the video https://www.the-digital-life.com/webserver-linux/ to run the simple docker compose file. I made edits to the PUID, TZ, URL & SUBDOMAINS of my docker-compose.yml file. When I ran it it failed. I checked my ec2, it was up. I checked linode to make sure that the CNAME information was correct. I circled back to DNS propagate to ensure that was right. In my local computer I ran nmap -Pn -p 80,443 rindata-staging.pulitzercenter.org to check if the ports were open. They were. In my configuration file my friend realized that I had my URL as my subdomain and my subdomain as www, dont ask me why- it\u2019s what the kind gentlemen in the video did so I copied. Additionally, the video only showed basic .yml file, I needed to add an additional option here ONLY_SUBDOMINAS=true to make sure that it did not generate a cert for the already certified domain. Once I made the URL my organizations main domain and the subdomain the staging url that I needed I ran docker-compose up and I was able to see that it was now working for the website. Granted it was just the swag landing page but this allowed me to see that everything was configured and I was now able to take that docker file and move it into the the service section of the Aleph docker file. https://docs.linuxserver.io/images/docker-swag here I could see all the other options that I could include, granted this is not so straight forward so I hope that this all makes sense. Side bar knowledge: A proxy server speaks to the client to determine what site they want to visit. Here the credentials for HTTPS access are secured and the location of where the user wants to visit is made. Once the information is exchanged with the proxy server- the sever then talks to the web via HTTP (its assumed that this is secure) and obtains the information needed to redirect the client to the website that they want to visit. NOTES ON CONFIGURATION OF THE NGINX, I was working within the default.conf I had to add to the server_name _.*; to reflect the subdomain of my instance. Then location / { include /config/nginx/proxy.conf; include /config/nginx/resolver.conf; set $upstream_app ui; set $upstream_port 8080; set $upstream_proto http; proxy_pass $upstream_proto://$upstream_app:$upstream_port; I had to make sure that these defaults stayed the same but I had to set the definition for $upstream_app to include the name of the service that was in my docker-compose.yml (The main one with all the services). To be ui, as listed in that docker file. I also had to see what port it was listening on and change the port number to reflect the same as that docker compose file. Additionally, I wanted to change the http to https but as we know that the proxy will talk to the web in unencrypted (we assume its secure) there was no need to change this. The last line uses the defined values in the line above to create the definition so nothing was needed to change. As we are only working with the ui for now I removed the other portions of the sample document. The same document that i pulled was from the nginx proxy-sites, there were loaded subdomain files which i used the structure for to create. When we ran docker-compose up, the site was running but it gave us a 500 error. When we looked at the logs, we were able to see that this was within the api and not the UI so this let us know that we have the UI working well. When i typed the staging website followed by /api I was able to get to the JSON file even though there was an internal server error. Adding a user to Aleph: I had to run docker-compose up in one terminal window, open another and dun docker pc to see the running container. As the docker was running I then ran docker exec -it [CONTAINER ID] bash. I wont lie, i went down all the services until the command aleph create user worked. I had to run aleph \u2014- help to see the options that I had for creating other users, once i did that- I was able to sign into Docker. https://geekflare.com/run-commands-inside-docker/ After adding a user I needed to make sure that the service would not go down and would restart automatically without me having to run the docker-compose up to run the service aleph. In order to do that I needed to create a document within the /etc/systemd/system Once I created the alpeh.service file I needed to reload the systemctl with systemctl daemon-reload so that I could run the following commands service aleph start & service aleph stop . Debugging why the notifications and alerts are not working for Aleph: I created a new email account for the staging so that the aleph.env file would have the correct input within the alerts and notification section of the document. The first issue was that I made changes to the aleph.env file but I did not restart the service for those changes to take effect. In the previous section because I made changes to the systemctl I could now run service aleph stop & service aleph start to restart the Aleph. After doing that, I visited my staging website & tried to trigger an alert. Nothing happened. Went back to the aleph.env file & we saw that the TTLS & SSL set to true but the port that I had listed was 25. This creates conflict because port 25 by default is not encrypted. Yet, I had the encrypted variables set. I changed the port to 587 (a quick google search helped me for this: https://www.gmass.co/blog/gmail-smtp/ ) and restarted the service once more. Attempted and no luck. The logs needed to be looked at, to do this: docker logs -f config_api_1 & docker logs -f config_ui_1 and also docker logs -f config_worker_1 These were the services that run with aleph, so I loaded up the browser- triggered the event and we saw nothing. I saw a prompt of the email that should have sent but it did not work. Learning point : when I was looking at the logs, I saw some components that are important to understand. POST is an HTTP method GET is what shows when you're getting information/data from the web, like when using a form. HTTP 499, 200 these could be error codes or letting you know things are ok. Then there will be numbers which show the size of the content. After that, components of the web page like svg files, images etc. I then went to Aleph's git page to see if similar issues had been faced and nothing matched exactly what I had. On the web, I went to settings ---> More Tools ---> web developer tools ---> Network Inspector. Once this was loaded I performed the same action of trying to get an alert or notification from the UI and I saw that it was a 502 bad gateway. Next step is to reach out to Aleph directly. I heard back and they asked me if I had a mail sever set up. I checked it out and I realized that yet again I had conflicting information for my alpeh.env file. I changed my port to 465, tested the notification and checked the logs. There I saw that it was an TTLS was not configured. I changed that to false in my .env file and ran it again. I had an error on username and password, I amended my username to include the @[ORGNAME] and from there it worked! I also found this link to be more helpful. Once I was able to create my user and log into the system, I wanted to test out the notification settings to register a new user on the site. The registration email would come in but the link associated with the email pointed to an error / non existing page. I searched the github and checked for potential issues that another person may have faced. I found nothing. I reached out to the developers and at the moment they did not know how to configure it to stop giving the error. The issue was that the link was generating in one string that made no sense. Finally, a developer said that a / was missing. I needed to edit the aleph.env file. [INSERT PHOTO OF THE LINK] I went into the .env file and edited the ALEPH_UI_URL variable and added the / at the end of my custom URL. From there, I restarted the aleph service so that these changes could reflect. That resolved the link issue - a user could now register. Something to consider at this juncture: With the registration link just open to anyone who has access to the link this poses a vulnerability to unwanted users registered & access to our data. Setting up KeyCloak Looking at the statement above this is what lead me to keycloak. I wrote the developers asking about the cli command aleph createuser . I asked because the man page aleph --help showed only three commands but did not allow me to see all users or amend users. I wanted to control user access via the command line. When I wrote the developers, they admitted that the CLI tool is very bare bones and that Keycloak is what they used internally and what is recommend. First issue with this: I was reading Keycloak documentation it states that they use quay.io powered by RedHat. When I went to their github (yes,i am learning always go to the damn github) they listed jboss as the docker image to use. My friend suggested that we look at the last updates on each: Service Last Updated Quay 2 months ago Jboss 3 months ago bitnami 11 hrs ago (from time of writing) Based on the figured above I am going to go with Bitnami. But I am jumping ahead, true to form thee was another issue. Apologies for this going back and forth. Docker Space Prior to speaking to my friend, I hadn't considered the update times. So I was going with the github and trying to install the image by running: docker run jboss/keycloak this lead to a not enough space error [INSERT DOCKER NOT ENOUGH SPACE IMAGE] My first thought was that I need to increase the size of the disk. I went into amazon and clicked on my EC2 instance. From there clicked on storage - selected the drive in question (after confirming the name within the server)and I updated the storage from 8GB to 10GB. To do that, click on the drive in question, then select actions, from there select modify storage. I restarted the entire server with sudo restart . When it was up again, I restarted my docker service docker start & the aleph service aleph start . When I ran df -hT I saw that I was still at 8GB. I ran lsblk and I saw 10GB..confusion. My friend explained that I needed to update the partition because while I made the disk bigger, the partition still remained the same. I ran docker system prune this freed up a whopping 215 bytes of data. Not enough. I ran service docker stop & service docker status to verify that the service was inactive. Once confirmed I can now unmount the disk. I ran umount /dev/nvme2n1 then df -h to make sure it was truly unmounted. Then I ran resize2fs /dev/nvme2n1 but I got the error that the server wanted me to check the fs with e2fsck -f /dev/nvme2n1 . Finally, I ran resize2fs /dev/nvme2n1 & mount -a to mount it back up and I saw with df -h that size was changed. I restarted docker & aleph. Now I can move to adding keycloak. KeyCloak NGNIX File Right now Aleph is using port 8080. I cannot test keycloak within that port as its already in use. Once I amend my docker-compose.yml to add the keycloak components it will work. On the back end I had to log into linode and duplicate the A/AAA record to match that of the staging record that we already had. That way that server understands when you reach that IP it can be pointing either to data staging or the keycloak staging. I created a keycloak .conf file that matches the aleph.conf file within the site-conf folder in ngnix . I ran service aleph stop and restarted - I went to my keycloak address only to see that there was a 502 error (my aleph service was working). I needed to add the subdomain to the docker-compose.yml Next, I decided to keep the postgres service for aleph and the postgres service for keycloak as two separate services within the docker-compose file. I had to add several configurations after reading the bitnami documentation. At first I totally left out the post-gres information and the additional keycloak information. postgres-keycloak: image: docker.io/bitnami/postgresql:11 environment: # ALLOW_EMPTY_PASSWORD is recommended only for development. - ALLOW_EMPTY_PASSWORD=yes - POSTGRESQL_USERNAME=bn_keycloak - POSTGRESQL_DATABASE=bitnami_keycloak volumes: - 'postgres-keycloak-data:/bitnami/postgresql' keycloak: image: docker.io/bitnami/keycloak:15 depends_on: - postgres-keycloak environment: #ADMIN_CREDENTIALS - KEYCLOAK_CREATE_ADMIN_USER=true - KEYCLOAK_ADMIN_USER=systems - KEYCLOAK_ADMIN_PASSWORD=[PASSWORD] - KEYCLOAK_MANAGEMENT_USER=manager - KEYCLOAK_MANAGEMENT_PASSWORD=bitnami1 - KEYCLOAK_DATABASE_HOST=postgres-keycloak - KEYCLOAK_DATABASE_PORT=5432 - KEYCLOAK_DATABASE_NAME=bitnami_keycloak - KEYCLOAK_DATABASE_USER=bn_keycloak - KEYCLOAK_DATABASE_PASSWORD=[PASSWORD] - KEYCLOAK_DATABASE_SCHEMA=public - KEYCLOAK_JDBC_PARAMS=sslmode=verify-full&connectTimeout=30000 Once I added these configurations I restarted aleph and I was able to access key-cloak. Notes to add: when testing the local docker you need to run docker-compose up This was long & I will organize it better soon.","title":"Aleph"},{"location":"Ansible%20Vault/","text":"Task Playbooks that had .vault_pass.sh files with permissions of 644 needed to be updated to 655. Passwords stored here would be easy to decrypt in these playbooks. Lesson learned this explanation is mediocre at best because I am taking the notes from memory rather than having typed it in parallel as I worked. Will amend this for future tasks. read the steps with caution if you ever find yourself on this page for pass assistance. Steps When working with an organizations private/public git repo, its important to understand that these repo's need to be copied locally on to your machine. The most secure format is to SSH, be sure that your rdsa key is within the organizations LDAP, these brings ease & security when trying to access private severs, machines & services. Once the repo is local on your machine, insure that you have the .git file as this will contain the information needed for you to push pull merge & commit to the organizations repo. In each playbook, I searched for the .vault_pass.sh with a ls -la command. Once found, I checked for the permissions a 644 would read as [-rw-r-r] which is not what we want. Good practice to take note of all the playbooks that have the permissions set to this. I took notes in a note pad. Make a backup of of vault_pass.sh with the following command cp vault_pass.sh bk.vault_pass.sh . This covers you in the event something goes wrong with the file you're working on, the authenticity of the original remains. Pass reads from the vault_pass.sh file, this file should be pointing to the pass path so it can decrypt and show you your passwords. It should resemble something like pass show ${XXXX_PASS_PATH}reponame/department/vault With the vault_pass.sh pointing to the correct path you now have to find all the other secret files, using the tree command at the root of the folder will allow you to see easier where the path to these files are. Once found you can run: ansible-vault rekey --new-vault-password-file .vault_pass.sh --vault-password-file bk.vault_pass.sh directory/secrets.yml After this you can run pass show to secure that the passwords have been encrypted and are able to function with the changes made.","title":"Ansible Vault"},{"location":"Clinv/","text":"Clinv What is it? Clinv was created by my good friend and could be found here . Beware, the documentation is very bare but I am working on adding content based off of my use case for no0bs. Clinv was created to asset inventory on AWS. Current Issue I need to connect clinv to my aws. But how to do that? One must first download aws cli . More information can be found here . Running aws configure will have me at a prompt in which I will use my AWS credentials for the IAM key. The IAM key can be found within your security settings of your aws account.The password only shows once - so you will want to do it that one time. If you have forgotten the password, simply create a new one and add into aws configure or amend your /.aws/config file. To connect clinv with the recently created aws cli run clinv update to update and this will show you a layout of the items you have connected to your amazon. Clinv troubleshooting: My clinv update was not working so I needed to see the error. The output said that it was with the region not connecting. I needed to check my aws configure file. To do that I typed cd .aws from there I ran ls which showed me the contents of the .aws directory, within there I saw the config file. There I opened and realized I had Region: US East (N. Virginia) us-east-1. The region needed to be: us-east-1 Once that was changed I reran clinv update and it worked! Clinv was connected to my aws.","title":"Clinv"},{"location":"Cloudflare/","text":"Intro At time of writing my organization is bringing up the idea of using cloudflare as our CDN. I have no idea what this - below are my findings. Cloudflare Cloudflare is a free CDN. CDN stands for content delivery network. The idea behind a CDN is that the content is not stored in a single web host / server. With the CDN you're able to distribute that information across hundreds of data centers across the globe thus providing intelligent routing of content is delivered to site visitor. Because of the distribution this will give a faster experience to the end users. Pings As site users ping our server, if we only have one this will create high ping rates. A ping (Packet Internet or Inter-Network Groper) is a basic Internet program that allows a user to test and verify if a particular destination IP address exists and can accept requests in computer network administration. Pings serve two purposes: * verifying that the target host is available * determining round-trip time (RTT) or latency Plug-in's You can attach several plug-ins to Cloudflare, some include caching & image optimization. This of course increases the cost of Cloudflare. Security Measures With hosts using CDN's, they create robust databases that can protect a site from malicious actors. Setting up Cloudflare Cloudflare.com, sign up as normal with email and password Add your site Cloudflare will pull all DNS records that it found Log into your DNS and change the name servers Allow for Propagation","title":"Cloudflare"},{"location":"Coda/","text":"","title":"Using Coda"},{"location":"Commands/","text":"Commands I've learned","title":"Commands"},{"location":"Common%20Website%20Attacks/","text":"Intro Hosting a site that has tons of users at any given moment will undoubtably be tested by curious , malicious individuals. Web security is not something I am the most versed on but below are some notes I gathered. Attack Types Cross Site Scripting Cross-site scripting (XSS) is an exploit where the attacker attaches code onto a legitimate website that will execute when the victim loads the website. That malicious code can be inserted in several ways. Most popularly, it is either added to the end of a url or posted directly onto a page that displays user-generated content. In more technical terms, cross-site scripting is a client-side code injection attack. Client Side Code I will have to investigate this a bit further but client-side code is Javascript that runs on the users machine. The reason client side code is important is because it's usefulness with interactive webpages; interactive content runs faster and more reliably since the user\u2019s computer doesn\u2019t have to communicate with the web server every time there is an interaction. Browser-based games are one popular platform for client-side code, since the client-side code can ensure the game runs smoothly regardless of connectivity issues. Malicious actors can wrap code in <script> </script> tags which tells the web browser to run it as Javascript code. Cookies (yum) Cookies are temporary login credentials saved on a user\u2019s computer. For example when a user logs onto a site like Facebook, the site gives them a cookie so that if they close the browser window and go back to Facebook later that day, they are automatically authenticated by the cookie and won\u2019t need to login again. Prevention Some of the options available to prevent the above attack include but are not limited to: * avoiding HTML inputs * validating inputs (prevent users from posting data that doesnt meet criteria) * block cookies * Set WAF rules Cross Site Forgery Cross-Site Request Forgery (XSRF) is an attack that forces an end user to execute unwanted actions on a web application in which they\u2019re currently authenticated. With a little help of social engineering (such as sending a link via email or chat), an attacker may trick the users of a web application into executing actions of the attacker\u2019s choosing. If the victim is a normal user, a successful XSRF attack can force the user to perform state changing requests like transferring funds, changing their email address, and so forth. If the victim is an administrative account, XSRF can compromise the entire web application. Differences Both attack types sound similar in nature, and they are. Key differences are: XSS: has the user trusting a badly implemented website attacker injects a script to the trusted website users browser executes the attackers script #### XSRF * a badly implemented website trusts the user * attacker tricks web browser into issuing requests * website executes attackers requests SQL Injection SQL injection (SQLi) is a web security vulnerability that allows an attacker to interfere with the queries that an application makes to its database. It generally allows an attacker to view data that they are not normally able to retrieve. This might include data belonging to other users, or any other data that the application itself is able to access. In many cases, an attacker can modify or delete this data, causing persistent changes to the application's content or behavior. In some situations, an attacker can escalate a SQL injection attack to compromise the underlying server or other back-end infrastructure, or perform a denial-of-service attack. To put it simply, an attacker can leverage certain commands against the URL that the database queries in order to show database content that is meant to be hidden. File Inclusion This is one I understand in theory but here is what I have gathered. Web applications are written to request access to files on a given system, including images, static text, and so on via parameters. Parameters are query parameter strings attached to the URL that could be used to retrieve data or perform actions based on user input. The following graph explains and breaking down the essential parts of the URL. And because we addresses work like file paths, an attacker could manipulate the URL to the path for passwords on a given site to then have those credentials displayed.","title":"Website Attacks"},{"location":"Debian%2010/","text":"Switching to Debian Network error was the first thing I noticed during the installation wizard. iwlwifi-3168-24.ucode Solution found here","title":"Debian Install"},{"location":"Docker/","text":"Docker On my journey, I learned that my organization makes use of Docker often. Here are my notes on the tool. Why use it ? Sometimes OS\u2019s and services are not compatible which leads to issues libraries , OS, & dependencies that will not work together. Docker runs components, dependencies in its own containers. Just had to build the docker configuration once. Containers are completed isolated environments, they can have their own network interfaces, etc but they all share the same OS kernel. Docker uses LXC containers. OS have two things, an OS kernel and a set of software. The OS kernel interacts with the underlying hardware. Its the software above the OS kernel that makes them different. Could have different compilers, user interface etc. Sharing the underlying kernel, what does that mean? Docker can run any flavor of OS so long as they are all based on the same kernel. Each docker container has the additional software that makes these OS's different. You will not be able to run a window based container with an Linux OS kernel. When you install Docker on windows you work Linux container on Linux virtual machine on windows. Unlike hyper visors, Docker not meant to run different OS's, dockers package and ship them anywhere any time as many times as we want. Docker installs on the OS. Containers VS Virtual Machines Utilization is used higher disk space, CPU space when you use a virtual machine. Docker allows it to boot up in seconds. Docker has less isolation as more resources are shared. You can see Docker hosted on virtual docker. Products are containerized. If you need to run multiple you could, you just need to remember to add a front load balancer. If one fails you can delete it and re-download it. ## How to Use Docker? There is a dockerhub or dockerstore. Where you can use docker run [REGISTRY PRODCUT NAME] ex: docker run ansible Container VS Image An image is a package or template, like a VM template that is used to create one or more containers that are running instances of images that are isolated and have their own enviornments & set of processes. You can create your own Docker image and push it to the docker repository. Developers & Ops In the past Developers would create the requirements and a set of instructions on how hosts must be set up, what prerequists are to be installed on the host and how the dependencies are to be configured, etc. OPs teams would hit issues as they did not develop, so working with developers to resolve. With docker the operations and developers team work together to transform the guide into a docker file with both of their requirements.The image can now run on any host, and it will work anywhere when deployed in production. Getting Started Docker has two editions, community (free docker products), Enterprise (paid products). Community is available on cloud platforms and on all major OS (Linux, Mac,Windows). If on Windows/OS you will have to run a virtual machine for Linux. Install Go here I installed using the convientient script. $ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh get-docker.sh With the following command I am pullling an image from dockerup called whalesay sudo docker pull docker/whalesay To run this image I ran sudo docker run docker/whalesay cowsay [INPUT] (insert my hi friendu photo here) ## Docker Commands docker run runs a container from an image If the image is not found it will pull the image from Docker hub, for the first time. docker ps lists all the containers that are running some basic information about them- name, container ID, names, ports and status. Each container gets a random name and ID. docker ps -a lists all the containers including previously stopped or exited containers. docker stop [CONTAINER ID OR CONTAINER NAME] will stop the running container. docker rm [CONTAINER ID OR CONTAINER NAME] will remove the docker container compeltely. docker images lists available images and their sizes docker rmi [IMAGE NAME] removes the images, but you must stop and remove all dependent container before it can be removed. docker pull [IMAGE] only pull the image and not run the container containers are not meant to host OS's, they are designed to carry computations or run the images. A container only lives as long as the process is alive inside. docker exec [IMAGE NAME] [COMMAND] will run a command within the container. docker run -d [IMAGE NAME] will run the container detached. docker run --name [DESIRED NAME] [IMAGE] will run a container and name it your desired name docker run [IMAGE NAME] would run the latest image docker run [IMAGE NAME]:[VERSION NUMBER] this would be considered a tag and docker would know to pull an image of this version Docker containers by default do not listen to standard input. It runs in a noninteractive mode. To have input you must include the -i argument in your command, this will run docker in interactive mode. No prompts will run with the -i argument as the prompt is on the terminal. The prompt must be set by adding -it argument rather than just -i . -t stands for a psudo terminal. Every docker container gets assigned an IP by default, its internal and can only be accessed within the docker host. If you have an image running on an internal docker , users outside the internal docker cannot access. To ammend this you can use the IP of the docker host, but you have to map the port inside the docker container to a free port. *Need to review and rewrite port mapping section. docker run -p 38282:8080 kodekloud/simple-webapp:blue this command would run docker with the container host being 38282 on port 8080 with the tag blue for image simple-webapp Data is persistent in a docker container. All data is gone if you delete the container. Best practice is to map directories to the docker host. It will mount directories inside the container and stored on the external volume and remains even if you delete the docker container. docker run -v /opt/datadir:/var/lib/mysql mysql this command would mount it to the var lib and then its specified to the directory inside the container. This is important because if you delete the Docker container the information remains if mounted in another point. docker inspect [CONTAINER NAME OR ID] gives you properites on a running container. docker logs [CONTAINER ID OR NAME] shows you the logs (the content written to the standard out of the container) docker run -it this command attaches to the terminal and in interactive mode on the designated container. Enviornment Variables Found under the config line in the container.","title":"Docker"},{"location":"Gitea/","text":"Gitea I came across Gitea at my old organization. It was what the systems team used instead of Github. Gitea allows you to host on your own sever and is 100% customizable. Coming into my new organization, as I would be a systems team by myself I wanted to have something similar for my boss to track what I was doing and for me to maintain myself organized. Later I would out that my boss prefers to use Asana for tracking so this project has been put on pause; nonetheless I want it documented as it was a bit of an adventure. Starting My friend told me about Disroot which met my needs for a) anonymity b) ease of use. So I created an account here . After being verified I had my own cup of tea for Gitea- but I had to use Disroot as the middle man, for now. Custom Issue Templates In my old organization my team had a custom issue template that I really liked. With my new Gitea I realized that I had something similar but alas it was not the same. This revealed two problems: I needed to connect to Gitea to modify the files so that the web would reflect my configurations. How to connect to Gitea via my terminal? Problems Solved After doing some research & pulling from my memory bank I realized that I had to generate an SSH key, store it within the settings of my Gitea. This would come in hand when attempting to speak to the Disroot server that would then connect me to my Gitea. The next issue was actually verifying that the ssh key could speak to git and connect. You have to install git and use it in a terminal by example : cd /path/where/you/want/your/clone git clone git://user@host/path/to/git/repo.git I then went into my git-repo folder from there I saw that I had an old git so I had to remove with rm -rf .gitea so that it would clear everything out and there would not be conflict. Then I ran git clone with the SSH URL pulled from my gitea online. They were connected and I could run commands like git push , git pull etc. From there I needed to create the mkdir .gitea . When Gitea runs, it searches the configuration files for something called ISSUE_TEMPLATE if it doesn't find then it will continue to look the same as before. Once the issue template file is created you can add content from online templates. My friend shared the one he created which I used. Including it below for your own modification or starting point. --- name: \"Bug\" about: \"Maintenance of existing infrastructure\" --- name: \"Bug\" about: \"Maintenance of existing infrastructure\" labels: - \"OKR: Maintenance\" - \"Priority: High\" --- <!-- Write a meaningful description of the issue below this line. --> ## Validation Criteria <!-- Describe what do you expect to have once the task is done --> * [ ] ## Steps <!-- Describe the steps required to fulfill the validation criteria. They should be clear enough so that anyone of the team is able to follow them. --> * [ ]","title":"Gitea"},{"location":"Github%20GPG%20Error/","text":"","title":"Github GPG Error"},{"location":"Images/","text":"A Better Way to Adding Images At the time of writing, I knew nothing about how folks added images to their git pages, digital gardens etc. Trying to figure it out the way, I resorted to creating a flickr, pulling images from the web or my screenshots and uploading the link. I believe its in my Markdown Syntax page, but don't quote me. The reason I didn't want to host on flickr is because: * limit on image storage * creating yet another account, thus increasing my digital footprint * pulling images from the web is annoying * i didn't like my personal mistakes living on the web My friend taught me another way. In my currently structure I have a docs folder which contains the guts of my digital garden. To host my own images I need to go to my docs folder and create the img folder. To do that running the command within your path to docs : mkdir img . From that directory I can now store my images. But now that they have a place to live, I need to reference them so my mkdoc can pull the image to my ATW site. When referencing any image in a post it should be ! [ text?] (IMAGE NAME.JPG) . I need to double check this command by trying it out. Will update with any bugs I find. Test","title":"Images"},{"location":"Intro%20to%20Web%20Applications/","text":"What are web applications? Web applications are interactive applications that run on web browsers. Normally they adopt the client-server architecture to run and handle interactions. They have the website interface which is what the user see's and engages with, then there's the back end where the source code that runs on the servers live. Historic Past In web 1.0 it was static pages for everyone and appeared the same for everyone. Now Contrast to today where web pages are dynamic, improved interoperability and heavier emphasis on user experience (UX). Deeper reading found here & here Traits Web applications are platform-independent and can run on any browser on any operating system. The functions of a web application are executed remotely on the remote server (leaving the users hard drive free). This also improves version roll out, all users no matter where have access to all the applications because updates are stored on the web server. Native Operating systems are another type of web applications. IT allows to create custom experiences that go further than just the web browsers capabilities. Web Application Distribution THere are open source web applications used world wide. But there are other that are closed source and normally sent through a subscription plan. open source web applications Joomla , Wordpress . Will attempt to find more at another time. closed source web applications Wix , Shopify , DotNetNuke . Security Because a web application can is accessed by many in around the world, its essential to make sure the web application is secure. A deeper reading in web application security testing can be found here . Front end trinity vulnerabilities are: HTML CSS JavaScript Another vulnerability type is SQL injections that are typically tied to Active Directory. Common Flaws SQL Injection : allowing attackers to spoof identities, tamper with data or disclose / destroy system data. File Inclusion : allowing attackers to include a file that if modified correctly could lead to cross site scripting, denial of service and more. Unrestricted File Upload : files that have executable code into systems. Indirect Object Reference : allow an attacker to find a pattern to then extort. Web Application Layout Category Description Web Application Infrastructure the structure of required components such as a database needed for the application to run as intended. Web Application Components represents all the components that the web application might interact with. Divided into the UI/UX,Client and Server. Web Application Architecture Architecture comprises all the relationships between the various web application components. Web application infrastructure can be split into 4 main models: Client-Server One server Many Servers - One Database Many Servers - Many Databases Client-Server: the server hosts the web application and distributes it to any client that accesses it. Breakdown A client will visit a web URL, the server uses the main web application interface UI. The user will click a button or request a specific function, think adding an item to a cart, logging in, etc. The browser sends an HTTPS/HTTP request to the server which then takes that request and performs the necessary tasks. When the server has the required data it sends the results back to the client browser displaying the results in a human-readable way. One Server: Considered the riskiest design- the web application and their components including the database are hosted on a single server.If the server becomes compromised or goes down for any reason, all hosted web applications become entirely inaccessible until resolved. Many Servers - One Database: This model can allow several web applications to access a single database and to have access to the same data without syncing the data between them. The web applications can be replicated from one of the main applications (backup/primary) or they can be separate web applications that share common data. Many Servers-Many Databases: Databases hold different web application data. The web application can only access private data and only common data that is shared across web applications. It's also possible to host each web applications database on its separate database server. This method is best used for redundancy purposes. This requires the use of load balancers because it offers access control measures and proper asset segmentation. Web Application Architecture Layer Description Presentation Layer UI process components that enable communication with the application and the system. Client accesses via web browser and are returned to the server in the form of HTML,Javascript and CSS. Application Layer All web requests by the client are correctly processed. Criteria's are checked, includes authorization, privileges and data passed on to the client. Data layer Works with the application later to determine where the required data is stored and can be processed. Microservices Normally these are independent components of the web application that are typically programmed for one task only. These services can range to the following: registrations searching payments ratings reviews Microservices use what would be considered stateless, the request and response are independent. The reason for this is that the data is stored separately from its respective microservice. Microservices have easier scaling, faster development of applications , agility, reusable code & resilience. Serverless Cloud providers like Azure, AWS and GCP allow serverless architectures for paid fees. The web applications are stateless, running on computing containers like Docker. This allows for companies to have the flexibility to build and deploy applications and services without having to manage the infrastructure. Front End & Back End Front end refers to everything that the user is going to interact with on a website, this also goes hand in hand with the UX or user experience. Languages for front end are normally HTML, CSS, & Javascript. Back end of a web application drives the core web application functionalities which are executed on the back end server that then processes everything required for the web application to run correctly. Component Description Back end server hardware and OS's that host the components like Linux,Windows or containers Web server handles all the HTTP requests and connections like APache & NGINX Databases store and retrieve the web application data like MySQL MSSQL Development frameworks development frameworks are used to develop core web application, like PHP, C#,Python HTML URL Encoding, or percent-encoding. For a browser to properly display a page's contents, it has to know the charset in use. In URLs, for example, browsers can only use ASCII encoding, which only allows alphanumerical characters and certain special characters. Therefore, all other characters outside of the ASCII character-set have to be encoded within a URL. URL encoding replaces unsafe ASCII characters with a % symbol followed by two hexadecimal digits. For example, the single-quote character ''' is encoded to '%27', which can be understood by browsers as a single-quote. URLs cannot have spaces in them and will replace a space with either a + (plus sign) or %20. The World Wide Web Consortium (W3C) defines DOM as: \"The W3C Document Object Model (DOM) is a platform and language-neutral interface that allows programs and scripts to dynamically access and update the content, structure, and style of a document.\" The DOM standard is separated into 3 parts: Core DOM - the standard model for all document types XML DOM - the standard model for XML documents HTML DOM - the standard model for HTML documents CSS CSS defines the style of each HTML element or class between curly brackets {}, within which the properties are defined with their values (i.e. element { property : value; }). JavaScript JavaScript is usually used on the front end of an application to be executed within a browser. While HTML and CSS are mainly in charge of how a web page looks, JavaScript is usually used to control any functionality that the front end web page requires. JavaScript is also used to automate complex processes and perform HTTP requests to interact with the back end components and send and retrieve data, through technologies like Ajax. Sensitive Data Exposure On the client-side if attacked, they put the end-user in danger of being attacked and exploited if they do have any vulnerabilities. If a front end vulnerability is leveraged to attack admin users, it could result in unauthorized access, access to sensitive data, service disruption, and more. Sensitive Data Exposure refers to the availability of sensitive data in clear-text to the end-user. Source code is of a web page or the page source on the front of web application is the HTML source code not to be confused with the back end code that is typically only accessible on the server itself. You can view any websites page source by either right clicking and selecting page source or pressing ctrl + u . IF those options do not work you could use a web proxy like Burp suite. HTML Injection It is critical to validate and sanitize user input on both the front end and the back end of the user input. HTML Injection occur when unfiltered user input is displayed on the page. This can be either through retrieving previously submitted code, think a user comment from the back end of the database or directly displaying the unfiltered input through JavaScript on the front end. Web page defacing consists of injecting new HTML code to change the web pages appearance. Injecting a malicious link could be something like inserting the following line into the web page <a href=\"evil website\">Click Me</a> . This would direct the user to press click me which would take them else where to obtain their credentials. Cross-Site Scripting (XSS) However, XSS involves the injection of JavaScript code to perform more advanced attacks on the client-side, instead of merely injecting HTML code. There are three main types of XSS: Type Description Reflected XSS happens when user input is displayed on the page after processing (think search result/ error message) Stored XSS happens when user input is stored on the back end database and then displayed upon retrieval (think posts/comments) DOM XSS happens when user input is directly shown in the browser and is written to an HTML DOM (vulnerable username or page title) if you're able to input your payload into a web site to, for example, obtain a cookie value. That value can then be used to attempt to authenticate to the victims account. Cross-Site Request Forgery(CSRF) Utilizes front end vulnerability that is caused by unfiltered user input. It performs certain queries and API calls on a web application that the victim is currently authenticated to, allowing the attacker to perform actions as the authenticated user. Type Description Sanitization remove special characters and non-standard characters from user input before displaying or storing it Validation ensuring that submitted user niput matches the expected format, think email matching email format Web application firewall (WAF) filters, monitors and blocks HTTP traffic to and from a web service. Back End Servers A back end server is the hardware and operating system on the back end that hosts all of the applications necessary to run the web application. It's the real system running all of the processes and carrying all of the tasks that make up the entire web application. The back end server fits in the data access layer. There are 3 back end components: 1) web server 2) database 3) development frame work","title":"Introdcution"},{"location":"Markdown%20Syntax/","text":"Learning Markdown Why am I doing this? Much like my username, I am a complete no0b when it comes to all things git, linux, command line and tech in general. I've realized in my path of enlightenment there is not tons of information that starts from the bottom of the barrel. Most things I have seen start from a moderate level or they give you the steps expecting you to know certain steps...I am making this for folks that want to learn from scratch, consider this us learning together. Why should I learn markdown? In order to work on Git, your repos will most likely be made in markdown. Best practice in Git is to create a \u201cREADME.md\u201d file. This file normally tells the git user what the project is about, installation information, and much more. Markdown has two versions: rendered version (the pretty/ easy on the eyes) & the raw version (this will look like code). See the image below for what I mean about \"code\". yes i know the above image has incorrect syntax, i will show you why if you keep on reading! Markdown Preview Tools & one tip If you're not yet a seasoned professional with markdown, it\u2019s great to be able to see a preview of the work you\u2019re doing, live. Currently I am on a Mac (i know boo- but its what i have). I am using the Brackets open source tool in conjunction with the Brackets Mark Down Preview. This allows me to see what I am typing in real time. If don't get the extension you can preview it by clicking the lighting bolt icon and it will open up a chrome window for you to preview, pretty neat. Note that it only works with Chrome. Git has its own preview but its annoying to click between two tabs to see what you're doing as you type. File extension for markdown is \".md\". I will be looking into Ubuntu Options as well.For you windows users, I hate that OS so you're on your own- or write me and we can work together to find something! Markdown Syntax Titles & Subtitles If you're anything like me, you will find a cool project on Git and realize that (in some cases not all) that the publisher has made a complete mess of the page and its generally hard to read. I like things to be clean and clear. Dividing your README.md files into chapters (if you will) is a great way to do that. The hashtag # has to come first then a space then your title. No quotations \u2192 # your desired text - \u201c#\u201d creates your title (H1) - \u201c##\u201d creates your subtitle (H2) - \u201c###\u201d -\u201c######\u201d creates your other header types (H3-H6) Remember that space after the hashtag, spacing is key in markdown as I've learned. Paragraphs To write a paragraph(s), you simply have to write text underneath your title, there's no special spacing or characters needed. Depending on your markdown preview or where you're posting one thing to note is line breaks. Some places might as for a line skip to signal a line break. To break that down: if you are on line 2, and enter your text then press enter and continue the paragraph on line 3- your rendered version will not show the text spaced out, it would be one continuously line. To break that line, you have to skip a line and start, so that is 2, skip line 3, and continue your text on line 4. Your rendered version should show that desired line break. Lists If you want lists you can add those by using one of two symbols: + or - You can stack your lists by moving to the next line. If you want to indent, you put a space at the start of the line and enter either the + or -. See below for how that would look. Links & Images If you want to include links in your markdown files you enter the description within brackets \"[description here]\" then the URL between parenthesis \"( https://www.youtube.com )\". Note that there should not be space between the brackets and parentheses- it wont work if there is. Click me For images, the syntax its a little tricky to understand but bear with me. You're going to include an exclamation point followed by brackets with the words \"alt text\" then your image URL between parentheses. You can pull image URLs by clicking the image, right click and selecting view info, this is different per website, OS, etc. Will work on a break down a few later. In-line Code To use the in-line code you're going to want to use the tilde symbol, this should normally be under the esc key on your keyboard. I will let you figure that one out as homework! When you want your code to appear in a block you enter three tildes, then the code you would like, followed by three more tildes on the next line. See the example below If you want to simply make your command seen, you can wrap the command between two tildes. When I say wrap that means one at the start of the command and one at the end. Tables I read somewhere that tables are not normally supported in markdown but Git allows it, so I will show you how to do that. The easiest way is with a photo. Take note of the pipe being used between each column. Additional, see that in every line there is a space. In the second line(58) you can see that there are three dashes it has to be 3. Also notice the spacing before and after each word and pipe- this is important if you want it to work. Spicy Text If you want to add a little style to your text you can do the following Bold: wrap your word in two asterisks \"*\" Italics: wrap your word in three asterisks Strike out: wrap your word in two tildes \"~\"","title":"Markdown Syntax"},{"location":"Microsoft%20Credentials/","text":"Microsoft Mystery At my current organization, I attempted to gain admin access to our MS Office. I needed to find out how -these licenses came to be. I reached out to Tech Soup and found that if you go to tech soup , login, click my account, then click Request history, this will take you to another page. In that page you will see manage Microsoft requests. There you can see the details on the donations that Microsoft has given your organization. Seeing this confirmed that we do have Microsoft accounts. So when I went to https://admin.microsoft.com and tried to register- it stated that the domain was already in use. Confusion. Odd, because in our password manager there was no persons / accounts listed as admin. I reached out to MS support. They got back to me after a few emails and listed a user who did not even know they were admin. I asked that they try to login, or hit forgot password. The user did not know the account in which this was tied to (they attempted to use their work address- it did not work). MS support said that in their records the \"tenant\" was unmanaged, this was the silver lining in the solution. So I had to proceed with the following steps: From Microsoft Support I needed: 1. An email address with the domain where you can send and receive emails. 2. The DNS records associated with the domain. To become an Admin: First, go to https://powerbi.microsoft.com/en-us/ and select \"Start Free\" so that you have an Office 365 Business user account for this \"tenant\". Then under \u201cGetting started with Power BI\u201d click on \u201cTry Power BI For Free\u201d You will need to sign up using your email address with the domain. I used my IT Admin not my work email tied to me. No one wants bloat mail. Once you sign up for Power BI use those credentials to sign in at portal.office.com After logging in to portal.office.com with the same credential you used on the Power BI website,click on the yellow square in the upper left-hand corner and then the Admin icon app. If the Admin icon is not displayed right away, click the view all my apps option to see if that is there. Finally, select the Admin icon. You should have the \u201cBecome an Administrator\u201d. You will need to verify the DNS records and then your account will be elevated to \u201cGlobal Administrator\u201d. After completing steps 5, it took me to a page insert image of page Screen Shot 2021-07-19 at 12.24.15 PM that included the TXT name, TXT Value and TTL. I had no idea what this meant. I had to go to my DNS, but I thought because this was Google that held it I went there first. I was wrong- google is merely an email provider. I had to: check out- https://lookup.icann.org/lookup , here I was able to see that Linode was our DNS provider. So I went to Linode and there I saw that there was a Domain(s) tab. I clicked in there and saw that there was a space for TXT records, I entered the three items that were on MS and waited 10 minutes then clicked confirm on the Microsoft admin page. At that point, it was confirmed and I was made admin for the domain pulitzer center.org.","title":"Microsoft Credntials"},{"location":"Monica%20CRM/","text":"My friend helps me in more ways than I care to share but alas I cannot avoid. I have always taken notes on my contacts. There's a portion in your mobile devices that has \"Notes\" normally thats where I would store information. What I realized is that often times the information here would be deleted on accident, the space is open on my mobile phone and can be wiped easily. It's happened where I had extensive information on someone and it was gone. Talking with my friend I they mentioned Monica which is a personal CRM - or a contact management system. Installation: SSHd into server, created a new directory, created the docker file using version: \"3.4\" services: app: image: monica depends_on: - db ports: - 8080:80 environment: - APP_KEY= - DB_HOST=db - DB_USERNAME=monica - DB_PASSWORD=secret volumes: - data:/var/www/html/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=monica - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Note that the APP_KEY= variable has to be 32 characters long, no special symbols. When I ran docker-compose up initially I was using this: version: \"3.4\" services: app: image: monica:fpm depends_on: - db environment: - DB_HOST=db - DB_USERNAME=monica - DB_PASSWORD=secret volumes: - data:/var/www/html/storage restart: always web: build: ./web ports: - 80:80 depends_on: - app volumes: - data:/var/www/html/storage:ro restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=monica - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql note that this had a web variable that I could not use - it as I was not in need of a reverse proxy. Once I got the right docker I did not have the APP_KEY= variable set and I needed to go back into my docker-compose.yml to set and restart the service. But when I tried to close my docker containers I was getting an error message permission was denied and they could not be closed down. I tried to remove images, contains, prune nothing worked. I checked several Stock overflows and I found that by running sudo aa-remove-unknown I was then able to run docker-compose down . Once docker was down I was able to remove the container. I made the changes to the docker compose file and ran docker-compose up and the service was rendering at localhost:[PORT]","title":"Monica CRM"},{"location":"Mosyle%20Permissions/","text":"Mosyle Configuration Apologies I wish that I had more for this as there were many steps. I will look into the Admin guide and see what I can update when I have a moment. Issue But to jump into this specific situation. At my old organization, I wanted to reduce the IT Friction by rolling out an MDM for all company owned computers, this would allow me to push out updates, app control, remote wiping etc. When I came to my new organization I was approved for this. When enrolling the MDM, I wanted to make sure that the computers in my fleet could only be logged in via my organizations Domain. I needed to connect the MDM to Google, when I attempted within the settings of the MDM, Google blocked me. I could not connect SSO as it was blocked by Google Solution I needed to log into my Google Admin Suite. From there on the left hand side I needed to find \"Security\" and then click on \"API controls\". Once there, I needed to find & click \"Manage third-party app access\". And from the new page, click on \"Configure new app\". Then I needed to look for \"Mosyle\" and add it as \"Trusted\". Once that was done, I was able to go into the MDM settings and set the policy to allow SSO with Gmail for all the computers enrolled in the MDM.","title":"Moysle"},{"location":"Motion%20Eye/","text":"Security Camera There are companies such as SimpliSafe, Ring, Blink, Arlo etc. The problem for me is that one does not know where the videos & images are captured, stored and potentially sold and analyzed. The consumer places these units throughout their home and the amount of data collected could be immense. Because of this I wanted to see what solutions existed that could provide me the security I wanted + the privacy I needed. I have started working with Raspberry Pi in the past but I wanted to set up a project that was live and my own. I stumbled on two resources: 1 & 2 . I hit a snag on both because they relied on CV which appears to be a virtual environment from within the pi. I could not get my mkvirtualenvw up and could not debug. I did further research and found how to install the MotionEyeOS . Steps Taken You will need the following: - Raspberry Pi Z W - Camera (I used this ) - Power Cable ( Like this ) - MicroSD I pulled the MotioneyeOS image , the latest. Unzip and I placed that file on my desktop. I then opened Balena Etcher from here here ,plugged in my microsd. Balena is easy in the sense that it will automatically detect the correct SD. I selected the image, the SD and then hit Flash. I needed to get wifi into the OS, I did that via wpa_supplicant.conf . Opening a normal text editor I pasted the following into the document and renamed it wpa_supplicant.conf country=us update_config=1 ctrl_interface=/var/run/wpa_supplicant network={ scan_ssid=1 ssid=\"MyNetworkSSID\" psk=\"Pa55w0rd1234\" } The file was then placed into the SD (no specific location). Once that was done, I plugged the SD into the Pi, provided power and saw the MotionEye OS loading. My wifi settings loaded and I was able to access the admin console. To do that, be sure to know the IP address of the raspberry Pi. It showed me the IP address in the loading screen, I typed that in and was taken to the login screen. By default the credentials are username admin with no password. I logged in, set a password, changed my IP address and I was able to create my personal preferences on my camera. Note: this can only be accessed from within the local network, I do not need an advanced set up as of yet. I was able to set up alerts to go to my personal email address. So that I would not need to always be logged in within my local network while away. (This post would benefit from photos, I am working in- WIP)","title":"Security Camera"},{"location":"Music/","text":"Music In the spirit of removing proprietary items to the extent that my knowledge and ability allow, I deiced to switch to the old fashion mp3 player. I chose the Sony Walkman as it had the features I liked: Bluetooth Small in size Expandable Memory (256GB at the moment for my use) Simple Interface Comes with music library for Windows, but could also use simple drag and drop of files to transfer music wide array of file types supported Problem I had an old laptop that contained most of the music I've horded since middle school. I simply selected all the files and placed them in my SanDisk memory. About 730 of the files were corrupted, transferring over 0 bytes. Solution After sorting the files by size, I then created .opus files to form my play list. I realized that I had several dozen .opus files that my mp3 could not read. I had to batch convert. All over the web I found different types of answers all that were too complex. My friend attempted to help but I got a parse error, still need to debug this. But for now- to convert a directory of *.[m4a,opus,etc] files to another type of file using ffmpeg the following command works: for i in *.opus; do ffmpeg -i \"$i\" -b:a 192k \"$i%.*}.mp3\"; done The above command will take all .opus files and spit out individually named .mp3 files. Please do this in the directory where your files are Clean up I recommend doing a simple rm *.opus to remove all excess opus files post convert.","title":"Audio"},{"location":"Networking/","text":"Network Introduction A network is nothing two or more computers connected by a cable or a wireless radio connection so that they can exchange information. These computers are normally connected by a wire that's on a switch, the switch is normally connected to a patch panel in small and big businesses. Networks are designed to share files, resources and programs. LAN stands for local area network. WAN stands for wide area network. MAN stands for metropolitan area network. The network that contains the hard drives printers and other resources that are shared with network computers is a server . Any computer that is not a server is a client. A node is a device that's connected to the network. A node is the same as a computer. A packet is a message that is sent over the network from one node to another node. The packet includes the address of the node that sent the packet. Networking The internet protocol (IP) defines the format of IP addresses: four eight-bit numbers called octets who's decimal values rage from 0 to 255. Layers Definition Physical (1) mechanical & electrical details of network components, this includes cables, connectors and network interfaces. Data link (2) how devices are identified on the network, typically MAC addresses. Switches operate at this layer. Network (3) handles the routing of data across networks. Routers operate here. Transport (4) provides the reliable delivery of packets. Session (5) establishes sessions between network applications. Presentation (6) converts data so that systems that use different data formats can exchange information. Application (7) allows applications to request network services. a trick to remember this is the mnemonic is ***A***ll ***P***eople ***S***eem ***T***o ***N***eed ***D***ata ***P***rocessing","title":"Networking"},{"location":"Resources/","text":"Resources This will be a location for links of useful information I have found that has helped me with issues, questions or just curiosities. I am still trying to figure out the best structure - I understand its not the clearest at the moment. Bare with me - WIP. Websites I will include a quick scenario for which each link has helped me. W1 Backup Options https://www.howtogeek.com/427480/how-to-back-up-your-linux-system/ I had a corrupted motherboard. It could not hold the Windows OS (think blue screen of death). I scrapped that for Ubuntu, and it crashed about 8 times in the span of one month. It was the stable build. Now that I have a new machine, I want to not have to continue to download all the configurations I made. I was not able to perform it via rsync but I did use grsync. W2 Password Management I most often use KDBX file types for password storage such as Macpass for Mac OS & Keeweb for Linux/Windows OS. A shared/local & individual & within the terminal you can use: https://www.passwordstore.org/ The tutorial I used is: https://www.fossmint.com/pass-commandline-password-manager-for-linux/ ( also very glad to see it was a POC blogger ) W3 Playlists I found this series of playlists that talks Dockers, Helms, DecOps, Terraform & more. Check it out here: https://www.youtube.com/channel/UCoOq-DtESvayx5yJE5H6-qQ/playlists?view=1&sort=lad CompTIA A+ Playlist: https://www.youtube.com/playlist?list=PLc6LqxQFwub8Qac87gEF0MhSAxe5PqoNS W4 Short YouTube Videos 10 Basic linux network commands: https://www.youtube.com/watch?v=vq063fExJN8 W5 Github Pages This page shows cybersecurity newsletters to keep you up to date on the latest tools, blogs, conferences etc. https://github.com/TalEliyahu/awesome-security-newsletters Windows Specific If you wanted to see the stats on your background follow this link: https://github.com/TalEliyahu/awesome-security-newsletters Playlist showing how to use Intune: https://www.youtube.com/c/IntuneTraining W6 Linux Networking Linux network commands https://bytexd.com/19-commands-to-monitor-the-network-activity-in-linux/ W6 Fiber Optic How & what is fiber optic: https://www.reddit.com/r/networking/comments/3gx5dz/ysk_if_you_dont_about_fiber_optics_and_how_they/ W7 How does Encryption work? Check out this blog post here: https://www.cloudflare.com/en-ca/learning/ssl/how-does-public-key-encryption-work/ W8 Coding Fun This one was funny to be because I am always typing the wrong thing in the terminal. Check here: https://github.com/nvbn/thefuck I think I will test it out.","title":"Resources"},{"location":"Securedrop/","text":"SecureDrop At my current role, I am a SecureDrop (SD) admin. It was not known to me how intense this role would be and all the terminal jargon I would have to know. Because of this, I will also store here and share tips and tricks that I have learned throughout my 3 years working with the tool. What is it? SecureDrop is an open source whistle blower submission system that media organizations and NGOs can install to securely accept documents from anonymous sources. It was originally created by the late Aaron Swartz and is now managed by Freedom of the Press Foundation. SecureDrop is available in 20 languages. More info can be found here . For my day-to-day I check SD for anonymous submissions of potential news story leads. How does it work? Tails I will be limiting this section for security reasons. SecureDrop works with the The Amnesic Incognito Live System (tails). It\u2019s an OS system, similar to Linux, MacOS & windows. The beauty of it is that it boots from a USB, and wipes itself clean after each use. It works only with TOR browsers. Arguably considered the most way to secure yourself on the internet. Check out more on tails here . Tails works in conjunction with tor browsers that use .onion websites. Info on tor here The Computers There is an Admin Station & there is a Secure Viewing Station (svs) . The admin station has access to the internet, where I use tor to access \".onion\" sites. I pull attachments from said sites, load them to an encrypted USB. This USB is then taken to the SVS. Both machines utilize tails exclusively. The SVS is not connected to the internet. With the encrypted USB drive and another tails USB, the system boots. From there the contents of submissions and such are able to be viewed. The backbone of security for viewing submissions relies on long pass-phrases & PGP, check out the PGP documentation in my git for more on that. Maintenance of SD Being a sysadmin for SD requires understanding of the command line, SSH with sever(s), and the role of each USB. FPF attempts to make it easy with GUI options but I can attest that this does not always work. Command line When updating and you don\u2019t see it working with the GUI V2 & V3 Upgrade Recently it was announced that the support for V2 onions would cease. Earlier I mentioned onion (tor browsers). Check back there in case you need a refresher. This leaves V2 running instances vulnerable, eliminate the vulnerability by switching to V3. Steps Verify ssh config files for Mon and App (your servers) Every secure drop instance will have the Admin USB and the Journalist USB, for any major configurations to SD, it will run from the admin USB only. First, open the terminal navigate to the SD directory- enter cd ~/Persistent/securedrop then verify that in servers (app & mon) have the files for ssh, they are titled app-ssh-ths & mon-ssh-ths . If they are there, excellent. If not, contact FPF support ASAP . Check your git status Within the same cd ~/Persistent/securedrop , enter git status . This will have the output of where the head detached at x.x.x . You want to make sure that its at the latest version. Verify what servers app & mon are connected to Again, within the same directory type this command cat ~/Persistent/securedrop/install_files/ansible-base/app-source-ths . After this command is run there is a secret URL that should be displayed here. When I say secret its a URL made of tons of letters- this is not to be shared. In my case there was not a secret URL it was a plain IP address not good. What to do? Reconfigure the servers. Run ssh -vvv app ssh -vvv mon . This will produce a verbose output of the debug as its attempting to connect with the servers. Note that this point for me nothing was really showing beyond it not connecting. After that you have to reconfigure your admin USB stick. To reconfigure type ./securedrop-admin sdconfig . As a reminder this is all within the ~Persistent/securedrop directory. The sdconfig gets all the configurations ready to send to the servers. It will provide with an interactive CL prompt where you will go through a series of questions. In this section, we also pay close attention to the questions on enabling or disabling V2, V3. Be sure to click yes for V3. Keeping V2 on is optional, but less secure. After that completes, you can type cat ~/.ssh/config . This will show the ssh configuration file, and we should see the private onion URL rather than the plain IP address we were seeing earlier. Test ssh Connection To test your ssh connection to the severs enter ssh app & ssh mon . If your terminal switches to app@XXXX then you've successfully SSH'd. Back up the admin stick It's good practice to create back ups of the admin stick/ journalist stick(s) in case something goes wrong during an upgrade you can always roll back. To do that enter ./securedrop-admin backup . This command creates a tarball for the current settings. This will take a while, do not fret. Pushing the Configurations to the Servers So everything that was done so far was only done locally within the tails/admin USB. The severs have no idea, and they are still operating under the old configurations. To make sure the servers have the correct configurations installed you should do the following. Run ./securedrop-admin sdconfig \u2190 applies the changes locally once more Run ./securedrop-admin install \u2190 this will send the configurations made in the step above to the servers. End I would ensure that you still have ssh capabilities. See above. And finally I would make sure that you have access to both of your instances. If you enabled V3, there should be a new link you should take note of which will appear as you access the source and journalists interface on the tor browser!","title":"Securedrop"},{"location":"Server%20Wifi%20Install/","text":"Server Relocation I found myself having to relocate. This means the current DHCP settings that I had in the router that I could control are going out the window. To break that down I was able to log into my parents router and assign myself a specific IP so that when I SSH into my server I do not have to worry about the IP address changing. What is SSH? SSH stands for secure shell or secure socket. Its a cryptographic network protocol that allows two computers to communicate and share data. When SSHing into a server the command syntax will typically follow this structure: ssh [username]@[IP Address or hostname] The default port for SSH is normally 22, because this is common knowledge for security reasons its better to change the open port to another number so that it restricts access. If you opt to change the port number there is an argument that you have to include within your command. ssh -p [port number] [username]@[IP Address or hostname] Finally, sshing to a port for the first time (or more) will ask you for a key, you can refer to my post rsa$config . If you need to specify a specific key the command will change to: ssh -p [port number] -i [path/to/key] [username]@IP Address or hostname] Sever VS Wifi The servers that I have worked with have always been either - connected via ethernet - cloud based (in which the service provider handles the connection) With my current living situation I could not connect an RJ45 from the router to my server :( so I had to do it via wifi. I will go through the steps that I took to get there. I asked around my network of folks and no one knew how to connect a server to Wifi. My first steps from some reading: - determine if my apple mini came with a NIC (network interface card). This would speak to the router in the house and get me connected - if it didn't, purchase a network adapter To check if my apple mini came with a NIC I ran ip a and this listed out a ton of jibberish on what my server had. I saw that it had an eth0 but nothing starting with a w which is what would indicate something relating to wifi. I saw nonthing that looked close to that. That meant I had to by my adapter. Pro tip: ensure that the adapter is Linux compatible. I purchased this . Setting up the Server Warning: you will need to connect the server to LAN before you can connect to via the adapter. I asked if I could use a port on the router for a few hours. With that I was able to install NetworkManager . I found out later that network manager is older than netplan and I ended up using netplan anyway. I connected to LAN and then while in root I executed the command that came with the adapter from the vendor. sh -c 'wget deb.trendtechcn.com/install -O /tmp/install && sh /tmp/install' From there I went into the /etc/netplan/ folder and then created test.yaml . From there I made the following configurations: network: vesion: 2 renderer: NetworkManager wifis: w1x0ccf896ddf26: dhcp4: no addresses: [] gateway4: [] nameservers: addresses: [] access-points: SSID: password: That's a lot, let me break it down. The first three lines are there because they have to be - it sets the stage for netplan to read it. For wifis the w1x0cc... that was the name of the adapter I had plugged in. I got this ID by running ip a . If you're lost, you can unplug and run the command, then plug the adapter and replug it in. You will see the different item and use that as your ID. For dhcp4: you can keep it as yes, but because this is a sever I wanted a static IP so that I could always access it because I like to use an alias for ease of use (this is configured in the hosts file) In the addresses field I put the DHCP static IP followed by the /24. Including the brackets. Gateway was the LAN IP of the router, probably going to be 192.168.1.1 For the nameservers I used cloud flare as that is reliable but you can use anything you like - so long as its reliable. Under access-points I changed the SSID variable to my wifi name. Note: if you have spaces in your wifi name, wrap it with \" ` \". Then I entered the WPA2 credentials. After that I ran netplan generate this would pull up any errors. I had to run this a few times as I had some syntax issues. I then ran netplan apply to have the test.yaml settings apply. Note: it has to be .yaml not .yml . After that I had zero idea if it would work. I ran a ping google.com and started seeing packet transfers and I knew I was connected. I then went into my machine and attempted to SSH in, after changing my /etc/hosts file to the new IP. And I was able to get in. I used this video for reference. I hope you found this helpful.","title":"Adding Wifi to Local Server"},{"location":"VPN/","text":"VPN VPN and servers can be confusing, I am still confused by them today. Below are some notes that I have gathered. VPN If you ever have an issue with a connection one of the common places to check is your resolv.conf file. How? In your terminal with your editor of choice open resolv.conf , if you do not have this file the editor will automatically create it. You will want to add the name-server of the site you're trying to reach. Example: nameserver 172.X.XX.X.X When not needed feel free to comment it out. Example: #nameserver 172.X.XX.X.X The system wont read it. Once you have made this change, you will want to make sure that in your VPN client you go to the configuration settings and select something manual along the lines of \"allow changes manually to network changes\". ## External HardDrive When you have to move files, for example a tarball you need to execute the following command: tar -cvfz [NAME OF TARBALL].tar.gz [DESIRED LOCATION OF FILE] Connecting To Servers Working on systems will have you ssh'ing into several machines. At times you will have to copy over tarballs from one machine into yours or your desired external storage device. For this you will use the scp command. In some cases your machine will not have a direct path to your key when you must list it out the command is as follows: scp -p 443 -i [PATH TO KEY] This command lists the port also to copy and where the key is to connect to the server. An example of copying contents from one server to an external hard drive would be: scp -p 443 -i ~/.ssh/[CERTIFICATE LOCATION] USERNAME@SEVERNAME:[LOCATION OF TARBALL][LOCATION WHERE YOU WANT IT COPIED] If you're lost on where to find the file path to your external drive (if thats where you want to push documents) Mac: Open the Disk Utility in the /Applications/Utilities/ folder, click on the partition, choose Info, and look at the disk identifier. Example: `Volumes/\"My Passport for Mac\" Linux: To be continued Windows: if you click on computer, then click the external drive in question, at the top of the window it will show you the path. Example: `C: Users\\sunflowerno0b\\USB Yes, this is very unique and its not commonly found, normally you indicate the scp command connect to the server and location of where you want to contents copied. 2FA Reset for OpenVPN VPNs allow for secure access to certain sites for your systems team. For example some teams may need only internal members to access your git. Authentication to these VPN protected sites normally include: ssl certificate username & password MFA When MFA needs to be rest, you should always look at the documentation. To rest the 2FA for a specific user using OpenVPN: ssh into the VPN (1XX.XX.XXX.XX) go into the root directory cd / run find / | grep sacli cd into the scripts folder run sudo ./scali -- user [USERNAME] --KEY \"prop_google_auth\" -- value \"false\" UserProPut Note that this will disable the google authenticator for the specific user. run sudo ./sacli -- user[USERNAME] -- lock 0 Google AuthLock this will reset the Google Authenticator for the user to rescan upon signing in again. Further options can be explored here VPN & LDAP When configuring VPN that will use your teams LDAP it is best practices for the username to match the LDAP profile when creating the user in admin. Should a user lose or replace phone","title":"Sever"},{"location":"VS%20Code%20Install/","text":"What is it? VS code is a free open-source code editor that is available for all major platforms. I was attempting to run Git in a more traditional fashion with the command line but I found it hard to find a spell checker for vim. IF you're reading this, please feel free to drop tips on how to do this, also taking a mental note to find my own solution. In the interim, I am using VS code downloaded here . What I have learned is that you can use VS code to write in several languages with a nice UX. There is a built in terminal & you can connect it to your GitHub. How to Connect GitHub<->VS Code Once downloaded, if you click the branch icon on the left panel it will ask if you want to clone, create, import a repository. If you click import, it will give you the option to paste the link to you git. Go into your repo, and paste the HTTPS URL. From there, a pop up will prompt you to connect your VSCODE to your GIT. Once you login on GIT, it will link. You should get a successful output. Note: there is other forms, I am sure- but this very straight forward. VSCode Spell-Right I have tested VSCODE spell-check, while very easy to install, (its plug and play-it) does not provide case sensitive feedback for your languages, in my case- markdown. So, I have used VS Spell-right . Installation was a bit complicated on Debian, but I sorted it out. Steps Going into the extensions option The steps on the VS spell-right git states that you have to Create the dictionaries sub folder as it doesn't exist by default. Here Here","title":"VS Code"},{"location":"Web%20Services/","text":"Why? Systems is very vast, creating a space to show the things learned over time will be helpful in a reference point to find things when stuck. Hope this helps. VPN If you ever have an issue with a connection one of the common places to check is your resolv.conf file. How? In your terminal with your editor of choice open resolv.conf, if you do not have this file the editor will automatically create it. You will want to add the namesever of the site you're trying to reach. Example: nameserver 172.X.XX.X.X When not needed feel free to comment it out. Example: #nameserver 172.X.XX.X.X The system wont read it. Once you have made this change, you will want to make sure that in your VPN client you go to the configuration settings and select something manual along the lines of \"allow changes manually to network changes\". ## External HardDrive When you have to move files, for example a tarball you need to execute the following command: tar -cvfz [NAME OF TARBALL].tar.gz [DESIRED LOCATION OF FILE] Connecting To Servers Working on systems will have you remoting into several machines. At times you will have to copy over tarballs from one machine into yours or your desired external storage device. For this you will use the scp command. In some cases your machine will not have a direct path to your key when you must list it out the command is as follows: scp -p 443 -i [PATH TO KEY] This command lists the port also to copy and where the key is to connect to the server. An example of copying contents from one server to an external hard drive would be: scp -p 443 -i ~/.ssh/[CERTIFICATE LOCATION] USERNAME@SEVERNAME:[LOCATION OF TARBALL][LOCATION WHERE YOU WANT IT COPIED] If you're lost on where to find the file path to your external drive (if thats where you want to push documents) Mac: Open the Disk Utility in the /Applications/Utilities/ folder, click on the partition, choose Info, and look at the disk identifier. Example: `Volumes/\"My Passport for Mac\" Linux: To be continued Windows: if you click on computer, then click the external drive in question, at the top of the window it will show you the path. Example: `C: Users\\sunflowerno0b\\USB Yes, this is very unique and its not commonly found, normally you indicate the scp command connect to the server and location of where you want to contents copied. 2FA Reset for OpenVPN VPNs allow for secure access to certain sites for your systems team. For example some teams may need only internal members to access your git. Authentication to these VPN protected sites normally include: ssl certificate username & password MFA When MFA needs to be rest, you should always look at the documentation. To rest the 2FA for a specific user using OpenVPN: ssh into the VPN (1XX.XX.XXX.XX) go into the root directory cd / run find / | grep sacli cd into the scripts folder run sudo ./scali -- user [USERNAME] --KEY \"prop_google_auth\" -- value \"false\" UserProPut Note that this will disable the google authenticator for the specific user. run sudo ./sacli -- user[USERNAME] -- lock 0 Google AuthLock this will reset the Google Authenticator for the user to rescan upon signing in again. Further options can be explored here VPN & LDAP When configuring VPN that will use your teams LDAP it is best practices for the username to match the LDAP profile when creating the user in admin. Should a user lose or replace phone","title":"Web Services"},{"location":"rsa%20%26%20config/","text":"RSA & CONFIG In systems administration much of the work performed requires the sysadmin to ssh into a machine. Organizations, if prudent will need the users RSA keys in order to provide secure authentication. RSA works under asymmetric encryption with the user generating a private & a public key pair. These keys are later stored in locations such as GitHub, gitea etc. Generating an RSA key Reading material here .ssh/ Config The .ssh directory includes files used for secure shell protocol. It's a cryptographic network protocol for operating network services securely over an unsecured network. There will be times where the systems team makes changes to the security infrastructure. Changes can include but are not limited to: changing HA Proxy rules updates to the bastion VPN updates IP address changes Potential Problem User at my org was attempting to ssh into a machine but was getting an UNREACHABLE! => {\"changed\"}: false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host XXX.XX.X.XXX port 22: COnnection timed out\", \"unreachable\": true} Solution Thought Process First, I realized this user had been out for about two weeks, perhaps their key expired & needed updating. This was not the case; they key the user was using was the team key for a generic user, not specific. I had to attempt to recreate the issue (note my friend guided my hand through all this, always grateful for you\u2661). I asked user to attempt connecting but directing ssh to the user key with the following command: ssh -i ~/.ssh/KEYFILE User states that this did not work either. I next asked for user to print out the debug report with the command: ssh -vv -i ~/.ssh/KEYFILE The debug report stated that user was trying to connect with there ssh config but .*.conf matched no files User states that config file was up to date. But the systeam just made changes to the bastion within the time frame that the user was out. User pasted their config file. Here is where I compared it to my config file and noticed they were missing a line which included our internal IP Range for host, user, identity file for the proxyJump bastion . Once they added the file, it worked.","title":"RSA/config"}]}